#   Basic Ideas of Machine Learning

##  Underfitting and Overfitting
欠拟合指模型无法拟合数据中的重要模式，以至于模型预测精度低。过拟合指模型过度地拟合到了观测数据中噪声的部分，以至于在未观测的数据标签预测时出现较大偏差的现象。我们也常将训练与测试损失都较大的情况称为欠拟合，将训练损失小而测试损失大的情况称为过拟合。这两种情况虽然都表现为训练与测试误差都较大，但其成因有本质区别。过拟合可以通过增加迭代次数、调整学习率等方式缓解；而欠拟合必须通过分析数据分布后，更换更合适的模型来解决。下图是欠拟合、恰好拟合和过拟合时训练损失和测试损失的曲线。
![输入图片说明](/imgs/2023-11-06/cvvzAFku10pjD4Fy.png)

##  Regularization
如上可知，过拟合是由于模型的参数过于复杂所引起的，故我们希望对模型的参数施加一些限制，避免在训练的过程中出现过拟合的现象，像这样对参数的复杂度进行约束的方法，就称为正则化。

以线性规划问题为例，介绍正则化的思想，在考虑所有样本的平方误差总和的情况下，将线性规划的损失函数改为：
![输入图片说明](/imgs/2023-11-06/OLnFkvw1a2bnWCYX.png)
相比于原本的损失函数，新的损失函数多了与θ的L2范数的平方有关的项。当我们想要最小化损失函数的时候，很自然的就会想到要将参数θ的L2范数尽量降低，这样限制其复杂度。我们用新的损失函数重新求解线性回归问题，可以得出解为：
![输入图片说明](/imgs/2023-11-06/ZxydXgALLNhFD0Nb.png)
因此，只要正则化约束存在，θ在理论上就是有解的。像这样使用了L2范数的正则化方法就称为L2正则化，又称为岭回归。

除了L2范数，我们还可以用其他范数来进行正则化约束，例如L0范数和L1范数。L1正则化的解析求解较为困难，但是可以利用梯度下降法得到数值解。带有L1约束的线性回归方法又称作最小绝对值收敛和选择算子回归，简称 LASSO 回归。

能拟合到的模式的复杂程度是由模型复杂度决定的，所选的模型复杂度不够，无论怎样调试也无法拟合到重要的数据模式，也就无法解决欠拟合的问题。但是如果选择的模型复杂度较高，为了避免其过拟合到数据的噪声上，我们可以加入适当的正则化约束，故一般选择模型时，我们应该选择具有一定复杂度的模型，首先避免欠拟合的问题，然后再通过调整正则化的强度来控制过拟合。

##  Parameter and Hyperparameter
我们通常用一些参数来表示模型，例如线性回归中的θ、学习率η，亦或者是KNN中的K，以及正则化系数。正则化系数以“已知数”的身份出现，我们在训练开始前就将其定好，像正则化系数这样不通过模型训练优化，需要人为指定的参数就叫超参数。调整超参数的值的过程则叫做调参。下图为开发者为完成某项任务而建立模型的过程：
![输入图片说明](/imgs/2023-11-06/yikGf4esAolqz8JT.png)
首先我们应当根据任务的特点和数据分布设计合适的模型，然后再确定其超参数的值，超参数确定后，我们开始初始化模型参数，并不断的优化模型，得到最终的优化结果，在测试集上观察其表现。接下来我们调整超参数，然后重复上述过程，选出最符合我们要求的超参数和参数。

参数与超参数的数量都会影响模型的复杂度。参数通常影响模型在训练时消耗的时间，而超参数由于需要我们提前设置，无法在训练中优化，其影响的是我们寻找最优模型的时间，过多的超参数会使得调参的过程非常困难。因此，优秀的机器学习模型通常需要减少其中超参数的数量，或者让部分超参数对模型的影响较小，不需要太多调整就可以达到较好的效果。

##  Data set partitioning and cross validation

我们防止过拟合的手段除了有正则化以外还可以从数据集的角度来防止过拟合产生。我们通常采用人为构造“测试集”的方法，将数据集随机划分为训练集和验证集两部分，用验证集来代替测试集的作用。一个完整的模型训练流程如图：![输入图片说明](/imgs/2023-11-06/TpG81wOAohwFiwDz.png)
当我们通过使用不同的超参数训练出不同的模型后，我们可以观察这些模型在验证集上的效果，选出表现最好的模型。因为我们完全没有用到验证集的信息，故我们可以认为按照验证集上的表现所选择的模型，在测试集上也有接近的效果。这个思路中包含着两个假设：
 1. 模型在训练时没有用到验证集的信息
 2. 训练集与真实的测试集中的数据分布相同

在实践中，为了进一步消除数据分布带来的影响，我们在划分训练集和验证集时，通常采用随机划分的方式。如果数据集中的数据原本是有序的，随机划分就可以防止训练集中的样本只来自于样本空间中的一小部分。如果再考虑到随机划分也可能因为运气不好，恰好得到有偏差的分布，我们还可以采用交叉验证的方法来再加一重保险。具体思路见下图：
![输入图片说明](/imgs/2023-11-06/HY9fQuFfVlWOlsRA.png)




#  Logistic regression

##  Linear Model
对于二分类问题，与线性回归相同，我们同样的作线性假设，但是对于样本与θ的乘积则有两点不同。首先，该乘积是连续的，并不能拟合离散变量，其次，该乘积的取值范围是R，与我们期望的{0，1}相距很远，故我们引入阈值z，定义f如下：	
![输入图片说明](/imgs/2023-11-13/N5KaEGm0nXJZoCW5.png)
但是，我们通过之前对线性回归的学习我们可以知道，解析方法和梯度下降的方法都需要以函数的梯度作为基础，但是引入阈值的方法使f在阈值处出现了跳跃，不可导，在阈值之外的导数却始终为零。这样的分类很“硬”，f难以训练。

于是我们通过换一个角度看待二分类问题，把样本x的类别y看作是有 0 和 1 两种取值的随机变量，我们只需要判断在x的作用下，y等于1获0的概率之间的大小关系，再将x归为概率较大的一类即可，同样可以达到分类的效果。相比于硬分类，概率分布可以用连续函数建模，从而可以对f求梯度。概率分布的取值范围应当是{0，1}。因此，我们需要某种从到的映射来确保其取值范围相同。在实践中，我们通常采用逻辑斯谛函数，其定义为：
![输入图片说明](/imgs/2023-11-13/YGfECV7NGIUH7wnn.png)


##  Maximum Likelihood Estimation\
在确立了逻辑回归的数学模型之后我们需要确定优化目标，而对于概率分布的问题我们则常常使用最大似然估计的思想来优化模型，即寻找参数θ使得模型在训练数据上预测出正确的标签的正确率最大。

通过分析，如果假设样本之间是两两独立的，那么模型将所有样本的分类都预测正确的概率就等于单个样本概率的乘积：![输入图片说明](/imgs/2023-11-13/b0Rc34gZCMc4W9tk.png)
该函数也称为似然函数，同时，为了简化运算，我们一般将似然值取对数，也即是对数似然，将连乘转化为求和：
![输入图片说明](/imgs/2023-11-13/Fjl84wHi6ritkmDz.png)
由于对数函数是单增的，于是我们就要求对数函数取得最大值时的参数是θ，通过梯度下降算法以及引入正则化约束后完整的优化目标与迭代公式为：
![输入图片说明](/imgs/2023-11-13/oUqcunjn9ERucbTR.png)
对于多分类问题其实就是二分类的一般形式，我们通常采用柔性最大值函数来将线性部分的预测映射到多分类概率上。如果我们将多分类问题中的类别设置为2，多分类问题也就变为了二分类问题。


##  Evaluation indicators
我们如何判断分类模型的好坏，这就诞生了分类问题的评价指标这一问题，在我们的默认思维中，一般会通过比较正负类的概率大小给出最后的判断，这实际上隐含了“阈值为 0.5”的假设。但是实际场景中，这正类与负类往往并不对称，我们必须根据任务的特点设置合适的阈值。并且，只有正确率这一标准是不够的，还需要分别考虑正负类分别被错判的比例。

在机器学习中，我们常用混淆矩阵来统计不同分类的结果，下图是混淆矩阵的组成：![输入图片说明](/imgs/2023-11-13/yM0GZ5NAFV2NLHpl.png)

##  Model implementation
我们所用的数据集 lr_dataset.csv 包含了二维平面上的一些点，这些点按位置的不同分为两类。每条样本依次包含横坐标、纵坐标和类别标签。我们的任务是训练逻辑斯谛回归模型，使模型对点的类别预测尽可能准确。在导入数据集后，我们实现准确率和AUC两种评价指标,代码如下：
```
def acc(y_true, y_pred):
    return np.mean(y_true == y_pred)

def auc(y_true, y_pred):
    # 按预测值从大到小排序，越靠前的样本预测正类概率越大
    idx = np.argsort(y_pred)[::-1]
    y_true = y_true[idx]
    y_pred = y_pred[idx]
    # 把y_pred中不重复的值当作阈值，依次计算FP样本和TP样本数量
	# 由于两个数组已经排序且位置对应，直接从前向后累加即可
	tp = np.cumsum(y_true)
	fp = np.cumsum(1 - y_true)
	tpr = tp / tp[-1]
	fpr = fp / fp[-1]
	# 依次枚举FPR，计算曲线下的面积
	# 方便起见，给FPR和TPR最开始添加(0,0)
	 s = 0.0
	tpr = np.concatenate([[0], tpr])
	fpr = np.concatenate([[0], fpr])
	for i in range(1, len(fpr)):
		s += (fpr[i] - fpr[i - 1]) * tpr[i]
	return s
```
由于本章所用的数据集大小较小，我们不再每次取小批量进行迭代，而是直接用完整的训练集计算梯度。接下来，我们按照之前推导的梯度下降公式定义训练函数，并设置学习率和迭代次数，查看训练结果。代码如下：
```
# 逻辑斯谛函数
def logistic(z):
    return 1 / (1 + np.exp(-z))
def GD(num_steps, learning_rate, l2_coef):
    # 初始化模型参数
    theta = np.random.normal(size=(X.shape[1],))
    train_losses = []
    test_losses = []
    train_acc = []
    test_acc = []
    train_auc = []
    test_auc = []
    for i in range(num_steps):
        pred = logistic(X @ theta)
        grad = -X.T @ (y_train - pred) + l2_coef * theta
        theta -= learning_rate * grad
        # 记录损失函数
        train_loss = - y_train.T @ np.log(pred) \
                     - (1 - y_train).T @ np.log(1 - pred) \
                     + l2_coef * np.linalg.norm(theta) ** 2/2
        train_losses.append(train_loss / len(X))
        test_pred = logistic(X_test @ theta)
        test_loss = - y_test.T @ np.log(test_pred) \
                    - (1 - y_test).T @ np.log(1 - test_pred)
        test_losses.append(test_loss / len(X_test))
        # 记录各个评价指标，阈值采用0.5
        train_acc.append(acc(y_train, pred >= 0.5))
        test_acc.append(acc(y_test, test_pred >= 0.5))
        train_auc.append(auc(y_train, pred))
        test_auc.append(auc(y_test, test_pred))
    return theta, train_losses, test_losses, \
    train_acc, test_acc, train_auc, test_auc
```
最后，我们定义各个超参数，进行训练并绘制出训练损失、准确率和 AUC 随训练轮数的变化。实验结果如下：![输入图片说明](/imgs/2023-11-13/JMwxaJ0oCMP0cGvY.png)

##  Conclusion
通过这一部分的学习，我掌握了机器学习中的一些基本概念和思想，了解了欠拟合和过拟合的产生原因，以及通过正则化和划分数据集的方法缓解过拟合，模型的超参数会影响我们寻找最优模型的难度与时间，因此，在设计模型时要注意超参数的个数和对模型的影响。

逻辑回归，它是最具有代表性的机器学习分类模型，至今还在学术研究和工业落地场景中被广泛使用。逻辑回归具有较好的可解释性，它参数的绝对值大小和正负代表了对应的特征对于预测数据类别的重要性，在医学、营销学、金融学等领域广泛被用于目标归因。



